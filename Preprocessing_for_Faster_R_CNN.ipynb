{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMtP7FYElZTAAtPLJGDK0Q9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TharinsaMudalige/Neuron-Brain_Tumor_Detection_Classification_with_XAI/blob/Detection-Classficiation-CNN/Preprocessing_for_Faster_R_CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Libraries, Mount Google Drive and Extract Dataset"
      ],
      "metadata": {
        "id": "7YHU3kQPfzFY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "llmfUPbvfoxW",
        "outputId": "face72d6-e29d-4d05-ba1b-3daf56cae750"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# Import libraries\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import random\n",
        "from sklearn.model_selection import train_test_split\n",
        "from google.colab import drive\n",
        "import zipfile\n",
        "from lxml import etree\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Paths\n",
        "drive_path = \"/content/drive/My Drive/DSGP/Original Dataset.zip\"  # Path to your zip file\n",
        "original_dataset_dir = \"/content/original_dataset\"\n",
        "preprocessed_images_dir = \"/content/drive/My Drive/DSGP/Preprocessed Dataset/Preprocessed Images\"\n",
        "annotations_dir = \"/content/drive/My Drive/DSGP/Preprocessed Dataset/Annotations\"\n",
        "\n",
        "# Unzip dataset\n",
        "if not os.path.exists(original_dataset_dir):\n",
        "    with zipfile.ZipFile(drive_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(original_dataset_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define Image Enhancements"
      ],
      "metadata": {
        "id": "HDtv2hIegCT4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "CLAHE for Contrast Enhancement"
      ],
      "metadata": {
        "id": "VDXxE_i0gF40"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def apply_clahe(image):\n",
        "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
        "    enhanced = clahe.apply(gray)\n",
        "    return cv2.merge([enhanced, enhanced, enhanced])"
      ],
      "metadata": {
        "id": "8f4eVDC4gKSO"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gaussian Blur to Reduce Noise"
      ],
      "metadata": {
        "id": "uDegKLlOgNSM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def apply_gaussian_blur(image):\n",
        "    return cv2.GaussianBlur(image, (5, 5), 0)"
      ],
      "metadata": {
        "id": "Kiw46RIkgQlj"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Skull Stripping"
      ],
      "metadata": {
        "id": "CR7qtkWFgTGH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def skull_strip(image):\n",
        "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "    _, mask = cv2.threshold(gray, 10, 255, cv2.THRESH_BINARY)\n",
        "    mask = cv2.dilate(mask, None, iterations=2)\n",
        "    return cv2.bitwise_and(image, image, mask=mask)"
      ],
      "metadata": {
        "id": "oS63ZsFSgVqg"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Resize, Normalize and Padding"
      ],
      "metadata": {
        "id": "kfxJj_rAgY1Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Resize and Normalize"
      ],
      "metadata": {
        "id": "cPBmKDVbgcc2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def resize_and_normalize(image):\n",
        "    resized = cv2.resize(image, (256, 256))\n",
        "    normalized = resized / 255.0\n",
        "    return normalized"
      ],
      "metadata": {
        "id": "LMQEamRGgfRv"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pad to Maintatin Aspect Ratio"
      ],
      "metadata": {
        "id": "uOluJku-giFY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def pad_to_square(image):\n",
        "    h, w, _ = image.shape\n",
        "    size = max(h, w)\n",
        "    padded = np.zeros((size, size, 3), dtype=np.uint8)\n",
        "    padded[:h, :w, :] = image\n",
        "    return cv2.resize(padded, (256, 256))"
      ],
      "metadata": {
        "id": "tx19J5AJgl1_"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generate Bounding Boxes"
      ],
      "metadata": {
        "id": "_d6BeDv71tK4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_bounding_box(image):\n",
        "    # Placeholder: Use the center of the image for bounding box\n",
        "    h, w, _ = image.shape\n",
        "    x_min, y_min = int(w * 0.3), int(h * 0.3)\n",
        "    x_max, y_max = int(w * 0.7), int(h * 0.7)\n",
        "    return [(x_min, y_min, x_max, y_max)]"
      ],
      "metadata": {
        "id": "Z42D5tb11yRB"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Saving Bounding Boxes as Annotations"
      ],
      "metadata": {
        "id": "uO-CJDi310Pl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def save_annotations(image_path, bboxes, label, output_folder):\n",
        "    xml_root = etree.Element(\"annotation\")\n",
        "    etree.SubElement(xml_root, \"filename\").text = os.path.basename(image_path)\n",
        "\n",
        "    for bbox in bboxes:\n",
        "        obj = etree.SubElement(xml_root, \"object\")\n",
        "        etree.SubElement(obj, \"name\").text = label\n",
        "        bbox_elem = etree.SubElement(obj, \"bndbox\")\n",
        "        etree.SubElement(bbox_elem, \"xmin\").text = str(bbox[0])\n",
        "        etree.SubElement(bbox_elem, \"ymin\").text = str(bbox[1])\n",
        "        etree.SubElement(bbox_elem, \"xmax\").text = str(bbox[2])\n",
        "        etree.SubElement(bbox_elem, \"ymax\").text = str(bbox[3])\n",
        "\n",
        "    output_path = os.path.join(output_folder, f\"{os.path.splitext(os.path.basename(image_path))[0]}.xml\")\n",
        "    with open(output_path, \"wb\") as f:\n",
        "        f.write(etree.tostring(xml_root, pretty_print=True))"
      ],
      "metadata": {
        "id": "LA01D_H614PH"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Augmentation"
      ],
      "metadata": {
        "id": "Sk0GbZPAgoiy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def augment_image(image):\n",
        "    augmented = []\n",
        "    augmented.append(cv2.flip(image, 1))  # Horizontal Flip\n",
        "    angle = random.randint(-15, 15)  # Random Rotation\n",
        "    center = (256 // 2, 256 // 2)\n",
        "    M = cv2.getRotationMatrix2D(center, angle, 1.0)\n",
        "    augmented.append(cv2.warpAffine(image, M, (256, 256)))\n",
        "    return augmented"
      ],
      "metadata": {
        "id": "lFAPvnRKgqj8"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Handle Class Imbalance"
      ],
      "metadata": {
        "id": "2tcmaoSNgtnr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def handle_class_imbalance(images, target_count=100):\n",
        "    if len(images) < target_count:\n",
        "        diff = target_count - len(images)\n",
        "        images += random.choices(images, k=diff)\n",
        "    return images"
      ],
      "metadata": {
        "id": "Q3guLJjggwHo"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Split Dataset"
      ],
      "metadata": {
        "id": "v_ufPTt_gymc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def split_dataset(images):\n",
        "    return train_test_split(images, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "201dI1cQg0qZ"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocess and Save Dataset"
      ],
      "metadata": {
        "id": "1XEoLXlgg6Dt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_and_save_dataset():\n",
        "    for root, _, files in os.walk(original_dataset_dir):\n",
        "        images = [os.path.join(root, f) for f in files if f.endswith(('.png', '.jpg', '.jpeg'))]\n",
        "\n",
        "        if len(images) == 0:\n",
        "            print(f\"No images found in folder: {root}\")\n",
        "            continue\n",
        "\n",
        "        # Handle class imbalance\n",
        "        images = handle_class_imbalance(images, target_count=100)\n",
        "\n",
        "        # Split into train/test\n",
        "        train_images, test_images = split_dataset(images)\n",
        "\n",
        "        for split, split_images in [(\"train\", train_images), (\"test\", test_images)]:\n",
        "            for img_path in split_images:\n",
        "                # Determine relative folder structure\n",
        "                relative_path = os.path.relpath(img_path, original_dataset_dir)\n",
        "                subfolder = os.path.dirname(relative_path)\n",
        "\n",
        "                # Create output directories\n",
        "                image_output_folder = os.path.join(preprocessed_images_dir, split, subfolder)\n",
        "                annotation_output_folder = os.path.join(annotations_dir, split, subfolder)\n",
        "                os.makedirs(image_output_folder, exist_ok=True)\n",
        "                os.makedirs(annotation_output_folder, exist_ok=True)\n",
        "\n",
        "                # Load and preprocess image\n",
        "                img = cv2.imread(img_path)\n",
        "                img = skull_strip(img)\n",
        "                img = apply_gaussian_blur(img)\n",
        "                img = apply_clahe(img)\n",
        "                img = pad_to_square(img)\n",
        "                img = resize_and_normalize(img)\n",
        "\n",
        "                # Save preprocessed image\n",
        "                output_image_path = os.path.join(image_output_folder, os.path.basename(img_path))\n",
        "                cv2.imwrite(output_image_path, (img * 255).astype(np.uint8))\n",
        "\n",
        "                # Generate and save bounding box annotations\n",
        "                bboxes = generate_bounding_box(img)\n",
        "                save_annotations(output_image_path, bboxes, \"tumor\", annotation_output_folder)"
      ],
      "metadata": {
        "id": "sm8VDus7g87o"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run Preprocessing"
      ],
      "metadata": {
        "id": "kgA9WI02g_So"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run preprocessing\n",
        "preprocess_and_save_dataset()\n",
        "\n",
        "# Confirm completion\n",
        "print(\"Preprocessing complete! Images and annotations saved.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bjeoeyBthCvF",
        "outputId": "d3252f56-f511-4afd-b73c-029d38725d0a"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No images found in folder: /content/original_dataset\n",
            "No images found in folder: /content/original_dataset/Original Dataset\n",
            "No images found in folder: /content/original_dataset/Original Dataset/Tumor\n",
            "No images found in folder: /content/original_dataset/Original Dataset/Tumor/glioma\n",
            "Preprocessing complete! Images and annotations saved.\n"
          ]
        }
      ]
    }
  ]
}
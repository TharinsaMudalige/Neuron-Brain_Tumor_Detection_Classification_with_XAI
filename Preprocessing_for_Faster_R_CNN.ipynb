{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1COvZtUnm5HGJ3s-IvN0l-47yFRcBOCaY",
      "authorship_tag": "ABX9TyOen4bEUF7JPk7olnl5B+sW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TharinsaMudalige/Neuron-Brain_Tumor_Detection_Classification_with_XAI/blob/Detection-Classficiation-CNN/Preprocessing_for_Faster_R_CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import Libraries"
      ],
      "metadata": {
        "id": "_2DoiRAE3508"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install opencv-python-headless matplotlib pandas SimpleITK\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import xml.etree.ElementTree as ET\n",
        "from xml.dom import minidom\n",
        "import matplotlib.pyplot as plt\n",
        "import SimpleITK as sitk\n",
        "from google.colab import drive\n",
        "from sklearn.model_selection import train_test_split\n",
        "import zipfile\n",
        "import gc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mayb4_V037v3",
        "outputId": "ab1d223c-3285-43d2-bb78-26e5d2a2c74a"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: opencv-python-headless in /usr/local/lib/python3.11/dist-packages (4.11.0.86)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: SimpleITK in /usr/local/lib/python3.11/dist-packages (2.4.1)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.11/dist-packages (from opencv-python-headless) (1.26.4)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.55.7)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mount Google Drive"
      ],
      "metadata": {
        "id": "NxoKmQJc4CRe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c27_zTs74EPi",
        "outputId": "fcf03669-f93c-4278-9cb0-d6cff6514f03"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Folder Structure Setup"
      ],
      "metadata": {
        "id": "YiGAEEYA4HM8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_zip_path = \"/content/drive/My Drive/DSGP/DSGP_Dataset.zip\"\n",
        "extracted_dataset_path = \"/content/drive/My Drive/DSGP/original_dataset\"\n",
        "\n",
        "# Unzip dataset if not already extracted\n",
        "if not os.path.exists(extracted_dataset_path):\n",
        "    with zipfile.ZipFile(dataset_zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extracted_dataset_path)\n",
        "\n",
        "base_dir = '/content/drive/My Drive/DSGP/Preprocessed Dataset'\n",
        "\n",
        "# Create main folders\n",
        "folders = [\n",
        "    'Images/Train/Tumor',\n",
        "    'Images/Train/No_Tumor',\n",
        "    'Images/Val/Tumor',\n",
        "    'Images/Val/No_Tumor',\n",
        "    'Images/Test/Tumor',\n",
        "    'Images/Test/No_Tumor',\n",
        "    'Annotations/Train/Tumor',\n",
        "    'Annotations/Train/No_Tumor',\n",
        "    'Annotations/Val/Tumor',\n",
        "    'Annotations/Val/No_Tumor',\n",
        "    'Annotations/Test/Tumor',\n",
        "    'Annotations/Test/No_Tumor'\n",
        "]\n",
        "\n",
        "for folder in folders:\n",
        "    os.makedirs(os.path.join(base_dir, folder), exist_ok=True)"
      ],
      "metadata": {
        "id": "RN4kwr5I4LKn"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Annotation Creation"
      ],
      "metadata": {
        "id": "iaxNOgWL4ZAi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_xml(image_path, class_name, box, size):\n",
        "    root = ET.Element(\"annotation\")\n",
        "    ET.SubElement(root, \"filename\").text = os.path.basename(image_path)\n",
        "    size_elem = ET.SubElement(root, \"size\")\n",
        "    ET.SubElement(size_elem, \"width\").text = str(size[1])\n",
        "    ET.SubElement(size_elem, \"height\").text = str(size[0])\n",
        "    ET.SubElement(size_elem, \"depth\").text = \"3\"\n",
        "    obj = ET.SubElement(root, \"object\")\n",
        "    ET.SubElement(obj, \"name\").text = class_name\n",
        "    ET.SubElement(obj, \"pose\").text = \"Unspecified\"\n",
        "    bndbox = ET.SubElement(obj, \"bndbox\")\n",
        "    ET.SubElement(bndbox, \"xmin\").text = str(box[0])\n",
        "    ET.SubElement(bndbox, \"ymin\").text = str(box[1])\n",
        "    ET.SubElement(bndbox, \"xmax\").text = str(box[2])\n",
        "    ET.SubElement(bndbox, \"ymax\").text = str(box[3])\n",
        "    xml_str = ET.tostring(root)\n",
        "    xml_pretty = minidom.parseString(xml_str).toprettyxml()\n",
        "    return xml_pretty\n",
        "\n",
        "def find_tumor_bbox(img):\n",
        "    gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
        "    _, thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_OTSU)\n",
        "    contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "    if contours:\n",
        "        largest = max(contours, key=cv2.contourArea)\n",
        "        x, y, w, h = cv2.boundingRect(largest)\n",
        "        return [x, y, x + w, y + h]\n",
        "    return [0, 0, 0, 0]"
      ],
      "metadata": {
        "id": "wm3sXniG4fBF"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preprocessing Functions"
      ],
      "metadata": {
        "id": "KVy8Hy754iCg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def skull_stripping(img):\n",
        "    sitk_img = sitk.GetImageFromArray(img)\n",
        "    sitk_img = sitk.Cast(sitk_img, sitk.sitkFloat32)\n",
        "    mask = sitk.OtsuThreshold(sitk_img)\n",
        "    stripped_img = sitk.GetArrayFromImage(sitk.Mask(sitk_img, mask))\n",
        "    return cv2.normalize(stripped_img, None, 0, 255, cv2.NORM_MINMAX).astype(np.uint8)"
      ],
      "metadata": {
        "id": "aHWuPPp54kbH"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Normalization\n",
        "def normalize_image(img):\n",
        "    p2, p98 = np.percentile(img, (2, 98))\n",
        "    img = np.clip(img, p2, p98)\n",
        "    return cv2.normalize(img, None, 0, 255, cv2.NORM_MINMAX)"
      ],
      "metadata": {
        "id": "bVB9qIjI4pdy"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_pipeline(img_path, target_size=(256, 256)):\n",
        "    img = cv2.imread(img_path)\n",
        "    if img is None:\n",
        "        print(f\"Warning: Could not read image {img_path}. Skipping...\")\n",
        "        return None\n",
        "\n",
        "    if len(img.shape) == 2:\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_GRAY2RGB)\n",
        "    else:\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    skull_free = skull_stripping(img)\n",
        "    normalized = normalize_image(skull_free)\n",
        "    final_img = (normalized * 255).astype(np.uint8)\n",
        "    return cv2.resize(final_img, target_size)"
      ],
      "metadata": {
        "id": "mPKcdRTY4tve"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Augmentation"
      ],
      "metadata": {
        "id": "V1CO9mpZ4znv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def augment_image(img):\n",
        "    h,w = img.shape[:2]\n",
        "    center = (np.random.randint(w//4, 3*w//4),\n",
        "             np.random.randint(h//4, 3*h//4))\n",
        "    scale = np.random.uniform(0.9, 1.1)\n",
        "\n",
        "    M = cv2.getRotationMatrix2D(center, np.random.randint(-15,15), scale)\n",
        "    return cv2.warpAffine(img, M, (w,h))"
      ],
      "metadata": {
        "id": "qjxttOy342Ph"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preprocessing Function"
      ],
      "metadata": {
        "id": "BMZXlhys49LY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def process_dataset():\n",
        "    data = []\n",
        "    for folder_name in ['no tumour', 'tumour']:\n",
        "        class_path = os.path.join(extracted_dataset_path, folder_name)\n",
        "        class_label = 'No_Tumor' if 'no ' in folder_name else 'Tumor'\n",
        "\n",
        "        for root, _, files in os.walk(class_path):\n",
        "            for file in files:\n",
        "                if file.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "                    data.append({'path': os.path.join(root, file), 'class': class_label})\n",
        "\n",
        "    df = pd.DataFrame(data)\n",
        "\n",
        "    # Debugging: Check DataFrame content\n",
        "    print(\"First 5 rows of df:\", df.head())\n",
        "    print(\"Columns in df:\", df.columns)\n",
        "    print(\"Total Images Found:\", len(df))\n",
        "\n",
        "    if df.empty:\n",
        "        print(\"Error: No images found! Check dataset extraction path.\")\n",
        "        return\n",
        "\n",
        "\n",
        "    tumor_df = df[df['class'] == 'Tumor']\n",
        "    no_tumor_df = df[df['class'] == 'No_Tumor']\n",
        "\n",
        "    if len(tumor_df) < len(no_tumor_df):\n",
        "        tumor_df = tumor_df.sample(len(no_tumor_df), replace=True)\n",
        "    else:\n",
        "        no_tumor_df = no_tumor_df.sample(len(tumor_df), replace=True)\n",
        "\n",
        "    balanced_df = pd.concat([tumor_df, no_tumor_df])\n",
        "\n",
        "\n",
        "    train_df, temp_df = train_test_split(balanced_df, test_size=0.3, stratify=balanced_df['class'])\n",
        "    val_df, test_df = train_test_split(temp_df, test_size=0.5, stratify=temp_df['class'])\n",
        "\n",
        "\n",
        "    for split_df, split_name in zip([train_df, val_df, test_df], ['Train', 'Val', 'Test']):\n",
        "        for idx, row in split_df.iterrows():\n",
        "            try:\n",
        "                processed_img = preprocess_pipeline(row['path'])\n",
        "                if processed_img is None:\n",
        "                    continue\n",
        "\n",
        "                if row['class'] == 'Tumor' and split_name == 'Train':\n",
        "                    processed_img = augment_image(processed_img)\n",
        "\n",
        "                img_filename = f\"{split_name.lower()}_{idx}.png\"\n",
        "                img_save_path = f\"{base_dir}/Images/{split_name}/{row['class']}/{img_filename}\"\n",
        "                cv2.imwrite(img_save_path, cv2.cvtColor(processed_img, cv2.COLOR_RGB2BGR))\n",
        "\n",
        "\n",
        "                annotation_folder = os.path.join(base_dir, \"Annotations\", split_name, row['class'])\n",
        "                os.makedirs(annotation_folder, exist_ok=True)\n",
        "\n",
        "                bbox = find_tumor_bbox(processed_img) if row['class'] == 'Tumor' else [0, 0, 0, 0]\n",
        "                xml_content = create_xml(img_filename, row['class'], bbox, processed_img.shape)\n",
        "                xml_save_path = os.path.join(annotation_folder, f\"{img_filename.split('.')[0]}.xml\")\n",
        "\n",
        "                with open(xml_save_path, 'w') as f:\n",
        "                    f.write(xml_content)\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing {row['path']}: {str(e)}\")"
      ],
      "metadata": {
        "id": "LAfpXlQH5Cra"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run Preprocessing"
      ],
      "metadata": {
        "id": "PuOwoT_i5Djx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "process_dataset()\n",
        "print(\"Preprocessing complete! All files saved in:\", base_dir)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RzyodI0N5GXH",
        "outputId": "8c5a1c68-e933-48f4-83ed-e8788f29dc37"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First 5 rows of df:                                                 path     class\n",
            "0  /content/drive/My Drive/DSGP/original_dataset/...  No_Tumor\n",
            "1  /content/drive/My Drive/DSGP/original_dataset/...  No_Tumor\n",
            "2  /content/drive/My Drive/DSGP/original_dataset/...  No_Tumor\n",
            "3  /content/drive/My Drive/DSGP/original_dataset/...  No_Tumor\n",
            "4  /content/drive/My Drive/DSGP/original_dataset/...  No_Tumor\n",
            "Columns in df: Index(['path', 'class'], dtype='object')\n",
            "Total Images Found: 300\n",
            "Preprocessing complete! All files saved in: /content/drive/My Drive/DSGP/Preprocessed Dataset\n"
          ]
        }
      ]
    }
  ]
}
{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TharinsaMudalige/Neuron-Brain_Tumor_Detection_Classification_with_XAI/blob/Detection-Classficiation-CNN/Faster_R_CNN_for_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nSq2BZ4QGdxG"
      },
      "source": [
        "Import Required Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "NSw-lQTgGf1P",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "092eb3a9-0c48-439a-81a9-0b11c0cac355"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: albumentations in /usr/local/lib/python3.11/dist-packages (2.0.4)\n",
            "Requirement already satisfied: numpy>=1.24.4 in /usr/local/lib/python3.11/dist-packages (from albumentations) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.10.0 in /usr/local/lib/python3.11/dist-packages (from albumentations) (1.13.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from albumentations) (6.0.2)\n",
            "Requirement already satisfied: pydantic>=2.9.2 in /usr/local/lib/python3.11/dist-packages (from albumentations) (2.10.6)\n",
            "Requirement already satisfied: albucore==0.0.23 in /usr/local/lib/python3.11/dist-packages (from albumentations) (0.0.23)\n",
            "Requirement already satisfied: opencv-python-headless>=4.9.0.80 in /usr/local/lib/python3.11/dist-packages (from albumentations) (4.11.0.86)\n",
            "Requirement already satisfied: stringzilla>=3.10.4 in /usr/local/lib/python3.11/dist-packages (from albucore==0.0.23->albumentations) (3.11.3)\n",
            "Requirement already satisfied: simsimd>=5.9.2 in /usr/local/lib/python3.11/dist-packages (from albucore==0.0.23->albumentations) (6.2.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.9.2->albumentations) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.9.2->albumentations) (2.27.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.9.2->albumentations) (4.12.2)\n",
            "Requirement already satisfied: cloud-tpu-client in /usr/local/lib/python3.11/dist-packages (0.10)\n",
            "Requirement already satisfied: google-api-python-client==1.8.0 in /usr/local/lib/python3.11/dist-packages (from cloud-tpu-client) (1.8.0)\n",
            "Requirement already satisfied: oauth2client in /usr/local/lib/python3.11/dist-packages (from cloud-tpu-client) (4.1.3)\n",
            "Requirement already satisfied: httplib2<1dev,>=0.9.2 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client) (0.22.0)\n",
            "Requirement already satisfied: google-auth>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client) (2.27.0)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client) (0.2.0)\n",
            "Requirement already satisfied: google-api-core<2dev,>=1.13.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client) (1.34.1)\n",
            "Requirement already satisfied: six<2dev,>=1.6.1 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client) (1.17.0)\n",
            "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client) (3.0.1)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.11/dist-packages (from oauth2client->cloud-tpu-client) (0.6.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.11/dist-packages (from oauth2client->cloud-tpu-client) (0.4.1)\n",
            "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from oauth2client->cloud-tpu-client) (4.9)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client) (1.66.0)\n",
            "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<4.0.0dev,>=3.19.5 in /usr/local/lib/python3.11/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client) (3.20.3)\n",
            "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.11/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client) (2.32.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.4.1->google-api-python-client==1.8.0->cloud-tpu-client) (5.5.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.11/dist-packages (from httplib2<1dev,>=0.9.2->google-api-python-client==1.8.0->cloud-tpu-client) (3.2.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client) (2025.1.31)\n",
            "Collecting torch_xla[tpu]\n",
            "  Downloading torch_xla-2.6.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (21 kB)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from torch_xla[tpu]) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torch_xla[tpu]) (1.26.4)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from torch_xla[tpu]) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torch_xla[tpu]) (2.32.3)\n",
            "Collecting libtpu==0.0.7.1 (from torch_xla[tpu])\n",
            "  Downloading libtpu-0.0.7.1-py3-none-manylinux_2_27_x86_64.whl.metadata (201 bytes)\n",
            "Collecting tpu-info (from torch_xla[tpu])\n",
            "  Downloading tpu_info-0.2.0-py3-none-any.whl.metadata (3.7 kB)\n",
            "INFO: pip is looking at multiple versions of torch-xla[tpu] to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting torch_xla[tpu]\n",
            "  Downloading torch_xla-2.5.1-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (17 kB)\n",
            "  Downloading torch_xla-2.5.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (14 kB)\n",
            "  Downloading torch_xla-2.4.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: cloud-tpu-client>=0.10.0 in /usr/local/lib/python3.11/dist-packages (from torch_xla[tpu]) (0.10)\n",
            "  Downloading torch_xla-2.3.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (7.9 kB)\n",
            "  Downloading torch_xla-2.2.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (7.3 kB)\n",
            "  Downloading torch_xla-2.1.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (7.3 kB)\n",
            "\u001b[31mERROR: Cannot install torch-xla[tpu]==2.1.0, torch-xla[tpu]==2.2.0, torch-xla[tpu]==2.3.0, torch-xla[tpu]==2.4.0, torch-xla[tpu]==2.5.0, torch-xla[tpu]==2.5.1 and torch-xla[tpu]==2.6.0 because these package versions have conflicting dependencies.\u001b[0m\u001b[31m\n",
            "\u001b[0m\n",
            "The conflict is caused by:\n",
            "    torch-xla[tpu] 2.6.0 depends on libtpu-nightly==0.1.dev20241010+nightly.cleanup; extra == \"tpu\"\n",
            "    torch-xla[tpu] 2.5.1 depends on libtpu-nightly==0.1.dev20240916; extra == \"tpu\"\n",
            "    torch-xla[tpu] 2.5.0 depends on libtpu-nightly==0.1.dev20240916; extra == \"tpu\"\n",
            "    torch-xla[tpu] 2.4.0 depends on libtpu-nightly==0.1.dev20240612; extra == \"tpu\"\n",
            "    torch-xla[tpu] 2.3.0 depends on libtpu-nightly==0.1.dev20240322; extra == \"tpu\"\n",
            "    torch-xla[tpu] 2.2.0 depends on libtpu-nightly==0.1.dev20231130; extra == \"tpu\"\n",
            "    torch-xla[tpu] 2.1.0 depends on libtpu-nightly==0.1.dev20230825; extra == \"tpu\"\n",
            "\n",
            "To fix this you could try to:\n",
            "1. loosen the range of package versions you've specified\n",
            "2. remove package versions to allow pip to attempt to solve the dependency conflict\n",
            "\n",
            "\u001b[31mERROR: ResolutionImpossible: for help visit https://pip.pypa.io/en/latest/topics/dependency-resolution/#dealing-with-dependency-conflicts\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: torch_xla-1.13-cp310-cp310-linux_x86_64.whl is not a supported wheel on this platform.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        },
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'torch_xla'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-51d25c074357>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m# TPU-specific libraries for PyTorch XLA\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch_xla\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch_xla\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxla_model\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mxm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch_xla\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistributed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallel_loader\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch_xla'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "!pip install albumentations\n",
        "!pip install cloud-tpu-client\n",
        "!pip install torch_xla[tpu]  # install the necessary package for TPU support\n",
        "!pip install cloud-tpu-client==0.10 https://storage.googleapis.com/tpu-pytorch/wheels/torch_xla-1.13-cp310-cp310-linux_x86_64.whl\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import cv2\n",
        "import xml.etree.ElementTree as ET\n",
        "import albumentations as A\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "from torchvision.models.detection import FasterRCNN\n",
        "from torchvision.models.detection.rpn import AnchorGenerator\n",
        "from torchvision.models.detection.backbone_utils import resnet_fpn_backbone\n",
        "\n",
        "# TPU-specific libraries for PyTorch XLA\n",
        "import torch_xla\n",
        "import torch_xla.core.xla_model as xm\n",
        "import torch_xla.distributed.parallel_loader as pl\n",
        "\n",
        "# Mount Google Drive later in the file\n",
        "from google.colab import drive"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dyoe-AgeGj2t"
      },
      "source": [
        "Define File paths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FlaktKDXGmQk"
      },
      "outputs": [],
      "source": [
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define dataset paths\n",
        "DATASET_PATH = \"/content/drive/MyDrive/DSGP/Preprocessed_Dataset\"\n",
        "TRAIN_PATH = os.path.join(DATASET_PATH, \"Train\")\n",
        "TEST_PATH = os.path.join(DATASET_PATH, \"Test\")\n",
        "VAL_PATH = os.path.join(DATASET_PATH, \"Val\")\n",
        "\n",
        "# Check dataset structure\n",
        "if not os.path.exists(TRAIN_PATH):\n",
        "    raise FileNotFoundError(f\"Train folder not found: {TRAIN_PATH}\")\n",
        "if not os.path.exists(TEST_PATH):\n",
        "    raise FileNotFoundError(f\"Test folder not found: {TEST_PATH}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Di4Evbsx6S1p"
      },
      "source": [
        "Define Custom Faster R-CNN Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JI5rlnhZKAvg"
      },
      "outputs": [],
      "source": [
        "class CustomFasterRCNN(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(CustomFasterRCNN, self).__init__()\n",
        "\n",
        "        # Use a ResNet50 backbone without pretrained weights\n",
        "        backbone = resnet_fpn_backbone('resnet50', pretrained=False)\n",
        "\n",
        "        # Adjusted anchor generator\n",
        "        rpn_anchor_generator = AnchorGenerator(\n",
        "            sizes=([32], [64], [128], [256], [512]),\n",
        "            aspect_ratios=([0.5, 1.0, 2.0],) * 5\n",
        "        )\n",
        "\n",
        "        # Define the Faster R-CNN model\n",
        "        self.model = FasterRCNN(\n",
        "            backbone,\n",
        "            num_classes=num_classes,\n",
        "            rpn_anchor_generator=rpn_anchor_generator\n",
        "        )\n",
        "\n",
        "    def forward(self, images, targets=None):\n",
        "        return self.model(images, targets)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uJqggka06aKI"
      },
      "source": [
        "Define Dataset Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZZsp80naKOO8"
      },
      "outputs": [],
      "source": [
        "class TumorDataset(Dataset):\n",
        "    def __init__(self, root_dir, transforms=None):\n",
        "        self.root_dir = root_dir\n",
        "        self.transforms = transforms\n",
        "        self.image_files = []\n",
        "        self.annotation_files = []\n",
        "        self.labels_set = set()\n",
        "\n",
        "        images_path = os.path.join(root_dir, \"Images\")\n",
        "        annotations_path = os.path.join(root_dir, \"Annotations\")\n",
        "\n",
        "        # Ensure folders exist\n",
        "        if not os.path.exists(images_path):\n",
        "            raise FileNotFoundError(f\"Missing 'Images' directory in {root_dir}\")\n",
        "        if not os.path.exists(annotations_path):\n",
        "            raise FileNotFoundError(f\"Missing 'Annotations' directory in {root_dir}\")\n",
        "\n",
        "        for tumor_type in sorted(os.listdir(images_path)):\n",
        "            tumor_images_dir = os.path.join(images_path, tumor_type)\n",
        "            tumor_annotations_dir = os.path.join(annotations_path, tumor_type)\n",
        "\n",
        "            if not os.path.isdir(tumor_images_dir) or not os.path.isdir(tumor_annotations_dir):\n",
        "                continue\n",
        "\n",
        "            # Sort files to maintain order\n",
        "            tumor_images = sorted(os.listdir(tumor_images_dir))\n",
        "            tumor_annotations = sorted(os.listdir(tumor_annotations_dir))\n",
        "\n",
        "            for image in tumor_images:\n",
        "                image_path = os.path.join(tumor_images_dir, image)\n",
        "                annotation_path = os.path.join(tumor_annotations_dir, os.path.splitext(image)[0] + \".xml\")\n",
        "\n",
        "                if os.path.exists(annotation_path):\n",
        "                    self.image_files.append(image_path)\n",
        "                    self.annotation_files.append(annotation_path)\n",
        "\n",
        "                    # Extract labels from annotation\n",
        "                    _, labels = self.parse_annotation(annotation_path)\n",
        "                    self.labels_set.update(labels)\n",
        "\n",
        "        # Create a fixed label mapping\n",
        "        self.label_dict = {name: idx + 1 for idx, name in enumerate(sorted(self.labels_set))}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_files)\n",
        "\n",
        "    def parse_annotation(self, annotation_path):\n",
        "        tree = ET.parse(annotation_path)\n",
        "        root = tree.getroot()\n",
        "\n",
        "        boxes = []\n",
        "        labels = []\n",
        "\n",
        "        for obj in root.findall(\"object\"):\n",
        "            name = obj.find(\"name\").text\n",
        "            labels.append(name)  # Tumor class name\n",
        "\n",
        "            bbox = obj.find(\"bndbox\")\n",
        "            xmin = int(bbox.find(\"xmin\").text)\n",
        "            ymin = int(bbox.find(\"ymin\").text)\n",
        "            xmax = int(bbox.find(\"xmax\").text)\n",
        "            ymax = int(bbox.find(\"ymax\").text)\n",
        "\n",
        "            boxes.append([xmin, ymin, xmax, ymax])\n",
        "\n",
        "        return np.array(boxes, dtype=np.float32), labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image_path = self.image_files[idx]\n",
        "        annotation_path = self.annotation_files[idx]\n",
        "\n",
        "        image = cv2.imread(image_path)\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        boxes, labels = self.parse_annotation(annotation_path)\n",
        "        labels = [self.label_dict[label] for label in labels]\n",
        "\n",
        "        target = {\n",
        "            \"boxes\": torch.tensor(boxes, dtype=torch.float32),\n",
        "            \"labels\": labels\n",
        "        }\n",
        "\n",
        "        if self.transforms:\n",
        "            # The 'class_labels' field is expected by albumentations\n",
        "            sample = self.transforms(image=image, bboxes=boxes, class_labels=labels)\n",
        "            image = sample[\"image\"]\n",
        "            target[\"boxes\"] = torch.tensor(sample[\"bboxes\"], dtype=torch.float32)\n",
        "\n",
        "        return image, target"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DAfjzpff6fx1"
      },
      "source": [
        "Define Data Transformations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gUsvCadHKUPa"
      },
      "outputs": [],
      "source": [
        "transform = A.Compose([\n",
        "    A.Resize(224, 224),\n",
        "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
        "    A.pytorch.transforms.ToTensorV2()\n",
        "], bbox_params=A.BboxParams(format='pascal_voc', label_fields=['class_labels']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6FEFCC6-6mN8"
      },
      "source": [
        "Load dataset and DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WYtpgp3FKbhc"
      },
      "outputs": [],
      "source": [
        "# Create Dataset and DataLoader\n",
        "train_dataset = TumorDataset(TRAIN_PATH, transforms=transform)\n",
        "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, collate_fn=lambda x: tuple(zip(*x)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2k3e19lU6tus"
      },
      "source": [
        "Initialize Model and Training Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "__PBNfa2KhkC"
      },
      "outputs": [],
      "source": [
        "# Set device to TPU using PyTorch XLA\n",
        "device = xm.xla_device()\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# Get number of tumor classes from annotations\n",
        "unique_classes = set()\n",
        "for annotation in train_dataset.annotation_files:\n",
        "    tree = ET.parse(annotation)\n",
        "    root = tree.getroot()\n",
        "    for obj in root.findall(\"object\"):\n",
        "        unique_classes.add(obj.find(\"name\").text)\n",
        "\n",
        "num_classes = len(unique_classes) + 1  # Tumor classes + background\n",
        "\n",
        "# Initialize the custom Faster R-CNN model and move it to the TPU\n",
        "model = CustomFasterRCNN(num_classes=num_classes)\n",
        "model.to(device)\n",
        "\n",
        "# Define optimizer\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0001)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yXMbUxA16zav"
      },
      "source": [
        "Train the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hMORaqHRKnbX"
      },
      "outputs": [],
      "source": [
        "num_epochs = 1  # Adjust the number of epochs as needed\n",
        "losses = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    # Wrap the DataLoader for TPU parallel loading\n",
        "    para_loader = pl.ParallelLoader(train_loader, [device])\n",
        "    for images, targets in para_loader.per_device_loader(device):\n",
        "        images = [image.to(device) for image in images]\n",
        "        # Convert target fields to tensors and move to TPU device\n",
        "        targets = [\n",
        "            {k: (torch.tensor(v).to(device) if isinstance(v, list) else v.to(device))\n",
        "             for k, v in t.items()} for t in targets\n",
        "        ]\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss_dict = model(images, targets)\n",
        "        loss = sum(loss for loss in loss_dict.values())\n",
        "        loss.backward()\n",
        "        xm.optimizer_step(optimizer, barrier=True)\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    losses.append(total_loss)\n",
        "    print(f\"Epoch {epoch+1}, Loss: {total_loss}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VHpF3VPR64G6"
      },
      "source": [
        "Plot Training Loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zZYYNviYKzYT"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(range(1, num_epochs + 1), losses, marker='o', linestyle='-', color='b')\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Training Loss\")\n",
        "plt.title(\"Training Loss Over Epochs\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T7Ilfk9I68U0"
      },
      "source": [
        "Save the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NEjmBZRj6-hK"
      },
      "outputs": [],
      "source": [
        "torch.save(model.state_dict(), \"/content/drive/MyDrive/DSGP/faster_rcnn_tumor_classification.pth\")\n",
        "print(\"Model saved!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RkCEwd2N7BOW"
      },
      "source": [
        "Evaluate the Model on Test Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FeAkQOlK7Eb1"
      },
      "outputs": [],
      "source": [
        "model.eval()\n",
        "sample_image_path = \"/content/drive/MyDrive/DSGP/CNN_dataset/Test/Images/astrocitoma/sample.jpg\"\n",
        "\n",
        "image = cv2.imread(sample_image_path)\n",
        "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "# Apply transform and add batch dimension\n",
        "image_transformed = transform(image=image)[\"image\"].unsqueeze(0).to(device)\n",
        "output = model(image_transformed)\n",
        "print(\"Inference output:\", output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h4mma8LB7HAI"
      },
      "source": [
        "Compute the IoU Score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IB0gK3xR7JI0"
      },
      "outputs": [],
      "source": [
        "# Function to Calculate IoU\n",
        "def calculate_iou(boxA, boxB):\n",
        "    \"\"\"Calculate Intersection over Union (IoU).\"\"\"\n",
        "    xA = max(boxA[0], boxB[0])\n",
        "    yA = max(boxA[1], boxB[1])\n",
        "    xB = min(boxA[2], boxB[2])\n",
        "    yB = min(boxA[3], boxB[3])\n",
        "\n",
        "    intersection = max(0, xB - xA) * max(0, yB - yA)\n",
        "    boxA_area = (boxA[2] - boxA[0]) * (boxA[3] - boxA[1])\n",
        "    boxB_area = (boxB[2] - boxB[0]) * (boxB[3] - boxB[1])\n",
        "    union = boxA_area + boxB_area - intersection\n",
        "\n",
        "    return intersection / union if union > 0 else 0\n",
        "\n",
        "# For demonstration, use the sample image annotation\n",
        "sample_annotation_path = sample_image_path.replace(\"Images\", \"Annotations\").replace(\".png\", \".xml\")\n",
        "boxes_gt, _ = train_dataset.parse_annotation(sample_annotation_path)\n",
        "\n",
        "ious = []\n",
        "# Compute IoU for each predicted box (here comparing to the first ground truth box for demonstration)\n",
        "for box in output[0]['boxes']:\n",
        "    iou = calculate_iou(box.cpu().detach().numpy(), boxes_gt[0])\n",
        "    ious.append(iou)\n",
        "\n",
        "# Plot IoU Scores\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.hist(ious, bins=20, color='g', alpha=0.7)\n",
        "plt.xlabel(\"IoU Score\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.title(\"IoU Score Distribution\")\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPUeYbLe1s4xW6NCDvBvaAE",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
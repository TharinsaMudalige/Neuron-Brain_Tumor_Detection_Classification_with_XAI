{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyO+JNDtmS2pjvnrbmoMgTae",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TharinsaMudalige/Neuron-Brain_Tumor_Detection_Classification_with_XAI/blob/Detection-Classficiation-CNN/Faster_R_CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import Required Libraries"
      ],
      "metadata": {
        "id": "nSq2BZ4QGdxG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U albumentations\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import cv2\n",
        "import xml.etree.ElementTree as ET\n",
        "import albumentations as A\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "from torchvision.models.detection import FasterRCNN\n",
        "from torchvision.models.detection.rpn import AnchorGenerator\n",
        "from torchvision.models.detection.backbone_utils import resnet_fpn_backbone"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NSw-lQTgGf1P",
        "outputId": "a1679f1a-24b6-42cc-a34b-086f9d47ed5a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: albumentations in /usr/local/lib/python3.11/dist-packages (2.0.4)\n",
            "Requirement already satisfied: numpy>=1.24.4 in /usr/local/lib/python3.11/dist-packages (from albumentations) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.10.0 in /usr/local/lib/python3.11/dist-packages (from albumentations) (1.13.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from albumentations) (6.0.2)\n",
            "Requirement already satisfied: pydantic>=2.9.2 in /usr/local/lib/python3.11/dist-packages (from albumentations) (2.10.6)\n",
            "Requirement already satisfied: albucore==0.0.23 in /usr/local/lib/python3.11/dist-packages (from albumentations) (0.0.23)\n",
            "Requirement already satisfied: opencv-python-headless>=4.9.0.80 in /usr/local/lib/python3.11/dist-packages (from albumentations) (4.11.0.86)\n",
            "Requirement already satisfied: stringzilla>=3.10.4 in /usr/local/lib/python3.11/dist-packages (from albucore==0.0.23->albumentations) (3.11.3)\n",
            "Requirement already satisfied: simsimd>=5.9.2 in /usr/local/lib/python3.11/dist-packages (from albucore==0.0.23->albumentations) (6.2.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.9.2->albumentations) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.9.2->albumentations) (2.27.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.9.2->albumentations) (4.12.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define File paths"
      ],
      "metadata": {
        "id": "Dyoe-AgeGj2t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define dataset paths\n",
        "DATASET_PATH = \"/content/drive/MyDrive/CNN_dataset\"\n",
        "TRAIN_PATH = os.path.join(DATASET_PATH, \"Train\")\n",
        "TEST_PATH = os.path.join(DATASET_PATH, \"Test\")\n",
        "VAL_PATH = os.path.join(DATASET_PATH, \"Val\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FlaktKDXGmQk",
        "outputId": "d1b1f5a7-7d1a-4f22-9428-60e775488cf0"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define Custom Faster R-CNN Model"
      ],
      "metadata": {
        "id": "Di4Evbsx6S1p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Faster R-CNN model from scratch\n",
        "class CustomFasterRCNN(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(CustomFasterRCNN, self).__init__()\n",
        "\n",
        "        # Use a ResNet50 backbone without pretrained weights\n",
        "        backbone = resnet_fpn_backbone('resnet50', pretrained=False)\n",
        "\n",
        "        # Define the model\n",
        "        self.model = FasterRCNN(\n",
        "            backbone,\n",
        "            num_classes=num_classes,\n",
        "            rpn_anchor_generator=AnchorGenerator(\n",
        "                sizes=((32, 64, 128, 256, 512),),\n",
        "                aspect_ratios=((0.5, 1.0, 2.0),) * 5\n",
        "            )\n",
        "        )\n",
        "\n",
        "    def forward(self, images, targets=None):\n",
        "        return self.model(images, targets)"
      ],
      "metadata": {
        "id": "JI5rlnhZKAvg"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define Dataset Class"
      ],
      "metadata": {
        "id": "uJqggka06aKI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TumorDataset(Dataset):\n",
        "    def __init__(self, root_dir, transforms=None):\n",
        "        self.root_dir = root_dir\n",
        "        self.transforms = transforms\n",
        "        self.image_files = []\n",
        "        self.annotation_files = []\n",
        "\n",
        "        images_path = os.path.join(root_dir, \"Images\")\n",
        "        annotations_path = os.path.join(root_dir, \"Annotations\")\n",
        "\n",
        "        # Check if the expected folders exist\n",
        "        if not os.path.exists(images_path) or not os.path.exists(annotations_path):\n",
        "            raise FileNotFoundError(f\"Expected 'Images' and 'Annotations' folders inside {root_dir}\")\n",
        "\n",
        "        # Iterate through each tumor category folder\n",
        "        for tumor_type in os.listdir(images_path):\n",
        "            tumor_images_path = os.path.join(images_path, tumor_type)\n",
        "            tumor_annotations_path = os.path.join(annotations_path, tumor_type)\n",
        "\n",
        "            if os.path.isdir(tumor_images_path) and os.path.isdir(tumor_annotations_path):\n",
        "                for image in os.listdir(tumor_images_path):\n",
        "                    image_path = os.path.join(tumor_images_path, image)\n",
        "                    annotation_path = os.path.join(tumor_annotations_path, os.path.splitext(image)[0] + \".xml\")\n",
        "\n",
        "                    if os.path.exists(annotation_path):\n",
        "                        self.image_files.append(image_path)\n",
        "                        self.annotation_files.append(annotation_path)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_files)\n",
        "\n",
        "    def parse_annotation(self, annotation_path):\n",
        "        tree = ET.parse(annotation_path)\n",
        "        root = tree.getroot()\n",
        "\n",
        "        boxes = []\n",
        "        labels = []\n",
        "\n",
        "        for obj in root.findall(\"object\"):\n",
        "            name = obj.find(\"name\").text\n",
        "            labels.append(name)  # Tumor class name\n",
        "\n",
        "            bbox = obj.find(\"bndbox\")\n",
        "            xmin = int(bbox.find(\"xmin\").text)\n",
        "            ymin = int(bbox.find(\"ymin\").text)\n",
        "            xmax = int(bbox.find(\"xmax\").text)\n",
        "            ymax = int(bbox.find(\"ymax\").text)\n",
        "\n",
        "            boxes.append([xmin, ymin, xmax, ymax])\n",
        "\n",
        "        return np.array(boxes), labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image_path = self.image_files[idx]\n",
        "        annotation_path = self.annotation_files[idx]\n",
        "\n",
        "        image = cv2.imread(image_path)\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        boxes, labels = self.parse_annotation(annotation_path)\n",
        "\n",
        "        # Convert labels to numbers\n",
        "        label_dict = {name: idx + 1 for idx, name in enumerate(set(labels))}\n",
        "        labels = torch.tensor([label_dict[label] for label in labels], dtype=torch.int64)\n",
        "\n",
        "        target = {\n",
        "            \"boxes\": torch.tensor(boxes, dtype=torch.float32),\n",
        "            \"labels\": labels\n",
        "        }\n",
        "\n",
        "        if self.transforms:\n",
        "            sample = self.transforms(image=image, bboxes=boxes, class_labels=labels)\n",
        "            image = sample[\"image\"]\n",
        "            target[\"boxes\"] = torch.tensor(sample[\"bboxes\"], dtype=torch.float32)\n",
        "\n",
        "        return image, target"
      ],
      "metadata": {
        "id": "ZZsp80naKOO8"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define Data Transformations"
      ],
      "metadata": {
        "id": "DAfjzpff6fx1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transform = A.Compose([\n",
        "    A.Resize(224, 224),\n",
        "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
        "    A.pytorch.transforms.ToTensorV2()\n",
        "], bbox_params=A.BboxParams(format='pascal_voc', label_fields=['class_labels']))"
      ],
      "metadata": {
        "id": "gUsvCadHKUPa"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load dataset and DataLoader"
      ],
      "metadata": {
        "id": "6FEFCC6-6mN8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = TumorDataset(TRAIN_PATH, transforms=transform)\n",
        "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, collate_fn=lambda x: tuple(zip(*x)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280
        },
        "id": "WYtpgp3FKbhc",
        "outputId": "3290ec77-20c1-49ba-c49a-5105db138455"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "Expected 'Images' and 'Annotations' folders inside /content/drive/MyDrive/CNN_dataset/Train",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-9928602b318f>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTumorDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTRAIN_PATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtrain_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollate_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-21ba43b0fa43>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root_dir, transforms)\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;31m# Check if the expected folders exist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages_path\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mannotations_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Expected 'Images' and 'Annotations' folders inside {root_dir}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;31m# Iterate through each tumor category folder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: Expected 'Images' and 'Annotations' folders inside /content/drive/MyDrive/CNN_dataset/Train"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initialize Model and Training Setup"
      ],
      "metadata": {
        "id": "2k3e19lU6tus"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Get number of tumor classes\n",
        "unique_classes = set()\n",
        "for annotation in train_dataset.annotation_files:\n",
        "    tree = ET.parse(annotation)\n",
        "    root = tree.getroot()\n",
        "    for obj in root.findall(\"object\"):\n",
        "        unique_classes.add(obj.find(\"name\").text)\n",
        "\n",
        "num_classes = len(unique_classes) + 1  # Tumor classes + background\n",
        "\n",
        "# Initialize custom Faster R-CNN model\n",
        "model = CustomFasterRCNN(num_classes=num_classes)\n",
        "model.to(device)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0001)"
      ],
      "metadata": {
        "id": "__PBNfa2KhkC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train the Model"
      ],
      "metadata": {
        "id": "yXMbUxA16zav"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 1\n",
        "losses = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for images, targets in train_loader:\n",
        "        images = list(image.to(device) for image in images)\n",
        "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "\n",
        "        loss_dict = model(images, targets)\n",
        "        loss = sum(loss for loss in loss_dict.values())\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    losses.append(total_loss)\n",
        "    print(f\"Epoch {epoch+1}, Loss: {total_loss}\")"
      ],
      "metadata": {
        "id": "hMORaqHRKnbX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plot Training Loss"
      ],
      "metadata": {
        "id": "VHpF3VPR64G6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(range(1, num_epochs + 1), losses, marker='o', linestyle='-', color='b')\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Training Loss\")\n",
        "plt.title(\"Training Loss Over Epochs\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "zZYYNviYKzYT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save the Model"
      ],
      "metadata": {
        "id": "T7Ilfk9I68U0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), \"/content/drive/MyDrive/DSGP/faster_rcnn_tumor_classification.pth\")\n",
        "print(\"Model saved!\")"
      ],
      "metadata": {
        "id": "NEjmBZRj6-hK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluate the Model on Test Data"
      ],
      "metadata": {
        "id": "RkCEwd2N7BOW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "image_path = \"/content/drive/MyDrive/DSGP/CNN_dataset/Test/Images/astrocitoma/sample.jpg\"\n",
        "\n",
        "image = cv2.imread(image_path)\n",
        "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "image_transformed = transform(image=image)[\"image\"].unsqueeze(0).to(device)\n",
        "output = model(image_transformed)"
      ],
      "metadata": {
        "id": "FeAkQOlK7Eb1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compute the IoU Score"
      ],
      "metadata": {
        "id": "h4mma8LB7HAI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to Calculate IoU\n",
        "def calculate_iou(boxA, boxB):\n",
        "    \"\"\"Calculate Intersection over Union (IoU).\"\"\"\n",
        "    xA = max(boxA[0], boxB[0])\n",
        "    yA = max(boxA[1], boxB[1])\n",
        "    xB = min(boxA[2], boxB[2])\n",
        "    yB = min(boxA[3], boxB[3])\n",
        "\n",
        "    intersection = max(0, xB - xA) * max(0, yB - yA)\n",
        "    boxA_area = (boxA[2] - boxA[0]) * (boxA[3] - boxA[1])\n",
        "    boxB_area = (boxB[2] - boxB[0]) * (boxB[3] - boxB[1])\n",
        "    union = boxA_area + boxB_area - intersection\n",
        "\n",
        "    return intersection / union if union > 0 else 0\n",
        "\n",
        "# Compute IoU\n",
        "ious = []\n",
        "for i, target in enumerate(output[0]['boxes']):\n",
        "    iou = calculate_iou(target.cpu().numpy(), train_dataset.parse_annotation(image_path)[0])\n",
        "    ious.append(iou)\n",
        "\n",
        "# Plot IoU Scores\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.hist(ious, bins=20, color='g', alpha=0.7)\n",
        "plt.xlabel(\"IoU Score\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.title(\"IoU Score Distribution\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "IB0gK3xR7JI0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
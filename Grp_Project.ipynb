{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TharinsaMudalige/Neuron-Brain_Tumor_Detection_Classification_with_XAI/blob/Image-Preprocesing/Grp_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9V1ts4DI0fpe"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o9IQfdkyBBxi",
        "outputId": "16f2dd68-8820-43a4-e66f-bca40a430fd0"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "iMz9L64F13Wj"
      },
      "outputs": [],
      "source": [
        "# Import Libraries\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import tensorflow as tf\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import train_test_split\n",
        "import random\n",
        "\n",
        "# Define preprocessing functions\n",
        "def resize_image(image, target_size=(256, 256)):\n",
        "    \"\"\"\n",
        "    Resize an image to the target size.\n",
        "    \"\"\"\n",
        "    return cv2.resize(image, target_size)\n",
        "\n",
        "def normalize_image(image):\n",
        "    \"\"\"\n",
        "    Normalize pixel values to [0, 1].\n",
        "    \"\"\"\n",
        "    return image / 255.0\n",
        "\n",
        "def augment_image(image):\n",
        "    \"\"\"\n",
        "    Apply random augmentation to an image.\n",
        "    \"\"\"\n",
        "    # Random horizontal flip\n",
        "    if random.random() > 0.5:\n",
        "        image = np.fliplr(image)\n",
        "\n",
        "    # Random rotation (between -10 and 10 degrees)\n",
        "    angle = random.randint(-10, 10)\n",
        "    h, w = image.shape\n",
        "    center = (w // 2, h // 2)\n",
        "    M = cv2.getRotationMatrix2D(center, angle, 1.0)\n",
        "    image = cv2.warpAffine(image, M, (w, h))\n",
        "\n",
        "    return image\n",
        "\n",
        "def preprocess_image(image, target_size=(256, 256), augment=False):\n",
        "    \"\"\"\n",
        "    Preprocess an image (resize + normalize + optional augmentation).\n",
        "    \"\"\"\n",
        "    image = resize_image(image, target_size)\n",
        "    if augment:\n",
        "        image = augment_image(image)\n",
        "    image = normalize_image(image)\n",
        "    return image\n",
        "\n",
        "# Define dataset splitting function\n",
        "def split_dataset(image_paths, train_ratio=0.7, val_ratio=0.15, test_ratio=0.15):\n",
        "    \"\"\"\n",
        "    Split the dataset into training, validation, and test sets.\n",
        "    \"\"\"\n",
        "    assert train_ratio + val_ratio + test_ratio == 1.0, \"Ratios must sum to 1.0\"\n",
        "\n",
        "    # Split into training and temporary sets\n",
        "    train_paths, temp_paths = train_test_split(image_paths, train_size=train_ratio, random_state=42)\n",
        "\n",
        "    # Split temporary set into validation and test sets\n",
        "    val_ratio_adjusted = val_ratio / (val_ratio + test_ratio)\n",
        "    val_paths, test_paths = train_test_split(temp_paths, train_size=val_ratio_adjusted, random_state=42)\n",
        "\n",
        "    return train_paths, val_paths, test_paths\n",
        "\n",
        "# Define function to preprocess and save images\n",
        "def preprocess_and_save(image_paths, output_dir, target_size=(256, 256), augment=False):\n",
        "    \"\"\"\n",
        "    Preprocess a list of images and save them to the output directory.\n",
        "    \"\"\"\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    for image_path in tqdm(image_paths):\n",
        "        # Load image\n",
        "        image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
        "        if image is None:\n",
        "            print(f\"Warning: Could not read image {image_path}. Skipping.\")\n",
        "            continue\n",
        "\n",
        "        # Preprocess image\n",
        "        preprocessed_image = preprocess_image(image, target_size, augment)\n",
        "\n",
        "        # Save preprocessed image\n",
        "        output_path = os.path.join(output_dir, os.path.basename(image_path))\n",
        "        cv2.imwrite(output_path, (preprocessed_image * 255).astype(np.uint8))\n",
        "\n",
        "# Define function to preprocess the dataset while preserving the original structure\n",
        "def preprocess_dataset(input_dir, output_dir, target_size=(256, 256), augment=False):\n",
        "    \"\"\"\n",
        "    Preprocess the entire dataset and save it in the output directory while preserving the original structure.\n",
        "    \"\"\"\n",
        "    # Collect all image paths\n",
        "    image_paths = []\n",
        "    for root, _, files in os.walk(input_dir):\n",
        "        for file in files:\n",
        "            if file.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.tif')):\n",
        "                image_paths.append(os.path.join(root, file))\n",
        "\n",
        "    # Split dataset\n",
        "    train_paths, val_paths, test_paths = split_dataset(image_paths)\n",
        "\n",
        "    # Preprocess and save each split while preserving the original structure\n",
        "    def save_images(paths, split_name):\n",
        "        for path in paths:\n",
        "            # Get the relative path to preserve the original structure\n",
        "            relative_path = os.path.relpath(os.path.dirname(path), input_dir)\n",
        "            output_subfolder = os.path.join(output_dir, split_name, relative_path)\n",
        "            os.makedirs(output_subfolder, exist_ok=True)\n",
        "\n",
        "            # Load and preprocess the image\n",
        "            image = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
        "            if image is None:\n",
        "                print(f\"Warning: Could not read image {path}. Skipping.\")\n",
        "                continue\n",
        "\n",
        "            # Apply augmentation only for the training split\n",
        "            preprocessed_image = preprocess_image(image, target_size, augment=(split_name == \"train\" and augment))\n",
        "\n",
        "            # Save the preprocessed image\n",
        "            output_path = os.path.join(output_subfolder, os.path.basename(path))\n",
        "            cv2.imwrite(output_path, (preprocessed_image * 255).astype(np.uint8))\n",
        "\n",
        "    # Save train, val, and test images\n",
        "    save_images(train_paths, \"train\")\n",
        "    save_images(val_paths, \"val\")\n",
        "    save_images(test_paths, \"test\")\n",
        "\n",
        "# Define input and output directories\n",
        "input_dir = \"/content/gdrive/MyDrive/Datasets\"  # Your dataset directory\n",
        "output_dir = \"/content/gdrive/MyDrive/Preprocessed_Datasets\"  # Directory to save preprocessed images\n",
        "\n",
        "# Preprocess the dataset with augmentation for training only\n",
        "preprocess_dataset(input_dir, output_dir, augment=True)"
      ],
      "metadata": {
        "id": "RcZWQE3S32se"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RViHIH2SGr-b"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1fVqyoQHbaPpjvI09Xrt43SbwgANTTeNc",
      "authorship_tag": "ABX9TyOlF7YCZVYOI6eZg9cZmrwY",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOzf6sFC5TdRrlU55ch8YT/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TharinsaMudalige/Neuron-Brain_Tumor_Detection_Classification_with_XAI/blob/Detection-Classification-VIT/Brain_Tumour_Classification_Using_VIT_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TDRADPlHGRPF"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Load and preprocess the dataset\n",
        "def load_dataset(base_dir):\n",
        "    \"\"\"Load images and labels from the dataset.\"\"\"\n",
        "    images = []  # List to store image data\n",
        "    labels = []  # List to store corresponding labels\n",
        "    class_names = sorted(os.listdir(base_dir))  # Get class names from folder names\n",
        "\n",
        "    for label, class_name in enumerate(class_names):  # Iterate over each class\n",
        "        class_dir = os.path.join(base_dir, class_name)  # Path to the class folder\n",
        "        for file in os.listdir(class_dir):  # Iterate over each image file in the folder\n",
        "            file_path = os.path.join(class_dir, file)  # Full path to the image file\n",
        "            # Load image as grayscale and resize to 224x224\n",
        "            image = tf.keras.preprocessing.image.load_img(file_path, color_mode='grayscale', target_size=(224, 224))\n",
        "            # Convert the image to a numpy array and normalize pixel values to [0, 1]\n",
        "            image = tf.keras.preprocessing.image.img_to_array(image) / 255.0\n",
        "            images.append(image)  # Add the image to the list\n",
        "            labels.append(label)  # Add the label to the list\n",
        "\n",
        "    return np.array(images), np.array(labels), class_names  # Return the images, labels, and class names\n"
      ],
      "metadata": {
        "id": "mvNPN8630iFG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define dataset directory\n",
        "base_dir = \"path/to/dataset\"  # Path to the dataset folder\n"
      ],
      "metadata": {
        "id": "GdQ2CQGr0n4u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset\n",
        "images, labels, class_names = load_dataset(base_dir)  # Load images, labels, and class names\n"
      ],
      "metadata": {
        "id": "JedXGsQN1WAu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the dataset equally for each class\n",
        "train_images, test_images, train_labels, test_labels = [], [], [], []  # Initialize empty lists for train/test splits\n",
        "for label in range(len(class_names)):  # Iterate over each class label\n",
        "    class_indices = np.where(labels == label)[0]  # Get indices of all images belonging to the current class\n",
        "    class_images = images[class_indices]  # Extract images for the current class\n",
        "    class_labels = labels[class_indices]  # Extract labels for the current class\n",
        "    # Split the class data into training and testing sets (80% train, 20% test)\n",
        "    train_x, test_x, train_y, test_y = train_test_split(class_images, class_labels, test_size=0.2, random_state=42)\n",
        "    train_images.extend(train_x)  # Add training images to the train list\n",
        "    test_images.extend(test_x)  # Add testing images to the test list\n",
        "    train_labels.extend(train_y)  # Add training labels to the train list\n",
        "    test_labels.extend(test_y)  # Add testing labels to the test list\n"
      ],
      "metadata": {
        "id": "fpA1IGEo1uEB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert lists to numpy arrays\n",
        "train_images = np.array(train_images)\n",
        "test_images = np.array(test_images)\n",
        "train_labels = np.array(train_labels)\n",
        "test_labels = np.array(test_labels)\n"
      ],
      "metadata": {
        "id": "u_f8VxOd5hkA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Create the Vision Transformer (ViT) model\n",
        "def create_vit_model(input_shape, num_classes):\n",
        "    \"\"\"Create a Vision Transformer model.\"\"\"\n",
        "    inputs = layers.Input(shape=input_shape)  # Input layer with the specified shape\n",
        "\n",
        "    # Patch embedding\n",
        "    patch_size = 16  # Size of each image patch\n",
        "    num_patches = (input_shape[0] // patch_size) * (input_shape[1] // patch_size)  # Calculate total number of patches\n",
        "    patches = layers.Conv2D(filters=64, kernel_size=patch_size, strides=patch_size, padding='valid')(inputs)  # Extract patches\n",
        "    patches = layers.Reshape((num_patches, -1))(patches)  # Reshape patches into 2D array for transformer\n",
        "\n",
        "    # Positional embedding\n",
        "    positional_embedding = layers.Embedding(input_dim=num_patches, output_dim=64)(tf.range(num_patches))  # Add positional info\n",
        "    x = patches + positional_embedding  # Add positional embeddings to the patch embeddings\n",
        "\n",
        "    # Transformer encoder layers\n",
        "    for _ in range(8):  # Add 8 transformer encoder layers\n",
        "        # Multi-head self-attention\n",
        "        attention_output = layers.MultiHeadAttention(num_heads=4, key_dim=64)(x, x)  # Apply attention mechanism\n",
        "        x = layers.Add()([x, attention_output])  # Add residual connection\n",
        "        x = layers.LayerNormalization()(x)  # Normalize the output\n",
        "\n",
        "        # Feed-forward network\n",
        "        ff_output = layers.Dense(128, activation='relu')(x)  # First dense layer with ReLU activation\n",
        "        ff_output = layers.Dense(64)(ff_output)  # Second dense layer without activation\n",
        "        x = layers.Add()([x, ff_output])  # Add residual connection\n",
        "        x = layers.LayerNormalization()(x)  # Normalize the output\n",
        "\n",
        "    # Classification head\n",
        "    x = layers.GlobalAveragePooling1D()(x)  # Global average pooling to reduce dimensions\n",
        "    outputs = layers.Dense(num_classes, activation='softmax')(x)  # Output layer with softmax activation\n",
        "\n",
        "    model = models.Model(inputs, outputs)  # Create the model\n",
        "    return model  # Return the model\n"
      ],
      "metadata": {
        "id": "sPmnh9ms5qF4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yZNnLJUZ50In"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TharinsaMudalige/Neuron-Brain_Tumor_Detection_Classification_with_XAI/blob/Detection-Classficiation-CNN/Generating_Annotations.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import Required Libraries"
      ],
      "metadata": {
        "id": "e8RAfeuzQfmu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install necessary libraries\n",
        "!pip install tensorflow tensorflow-hub tensorflow-addons opencv-python\n",
        "\n",
        "# Import libraries\n",
        "import os\n",
        "import random\n",
        "import shutil\n",
        "import uuid\n",
        "import cv2\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array, ImageDataGenerator\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from google.colab import drive\n",
        "from PIL import Image\n",
        "import xml.etree.ElementTree as ET\n",
        "from xml.dom.minidom import parseString"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r4j7zipaQhyp",
        "outputId": "c5304eed-d573-4548-969c-c1184c6717d8"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Requirement already satisfied: tensorflow-hub in /usr/local/lib/python3.11/dist-packages (0.16.1)\n",
            "Collecting tensorflow-addons\n",
            "  Downloading tensorflow_addons-0.23.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (4.11.0.86)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.25.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.70.0)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.26.4)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.12.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: tf-keras>=2.14.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow-hub) (2.18.0)\n",
            "Collecting typeguard<3.0.0,>=2.7 (from tensorflow-addons)\n",
            "  Downloading typeguard-2.13.3-py3-none-any.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.14.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.1.31)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
            "Downloading tensorflow_addons-0.23.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (611 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m611.8/611.8 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typeguard-2.13.3-py3-none-any.whl (17 kB)\n",
            "Installing collected packages: typeguard, tensorflow-addons\n",
            "  Attempting uninstall: typeguard\n",
            "    Found existing installation: typeguard 4.4.2\n",
            "    Uninstalling typeguard-4.4.2:\n",
            "      Successfully uninstalled typeguard-4.4.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "inflect 7.5.0 requires typeguard>=4.0.1, but you have typeguard 2.13.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed tensorflow-addons-0.23.0 typeguard-2.13.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mount Google Drive and Define paths"
      ],
      "metadata": {
        "id": "CJ49t6rEQjlw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define dataset paths\n",
        "RAW_DATASET_PATH = \"/content/drive/MyDrive/DSGP/DSGP_dataset\"\n",
        "OUTPUT_PATH = \"/content/drive/MyDrive/DSGP/CNN_Dataset\"\n",
        "\n",
        "# Detect tumor classes\n",
        "tumor_classes = [folder for folder in os.listdir(RAW_DATASET_PATH) if os.path.isdir(os.path.join(RAW_DATASET_PATH, folder))]\n",
        "print(\"Detected tumor classes:\", tumor_classes)\n",
        "\n",
        "# Create output folders\n",
        "for split in [\"Train\", \"Val\", \"Test\"]:\n",
        "    for subdir in [\"Images\", \"Annotations\"]:\n",
        "        for tumor_class in tumor_classes:\n",
        "            os.makedirs(os.path.join(OUTPUT_PATH, split, subdir, tumor_class), exist_ok=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BRGnu6YXQnkE",
        "outputId": "4b5227c9-d5e6-4a9d-ea74-5af3e4a6b05c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Detected tumor classes: ['tuberculoma', 'granuloma', 'no_tumour', 'papiloma', 'schwannoma', 'meduloblastoma', 'pituitary', 'neurocitoma', 'oligodendroglioma', 'meningioma', 'germinoma', 'astrocitoma', 'glioblastoma', 'ependimoma', 'ganglioglioma', 'carcinoma']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Build & Compile U-Net Model for Segmentation"
      ],
      "metadata": {
        "id": "DGBbOy3ooWN6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_unet(input_size=(256, 256, 3)):\n",
        "    \"\"\"Build a U-Net model for RGB image segmentation.\"\"\"\n",
        "    inputs = keras.Input(input_size)\n",
        "\n",
        "    # Encoder\n",
        "    c1 = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(inputs)\n",
        "    c1 = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(c1)\n",
        "    p1 = layers.MaxPooling2D((2, 2))(c1)\n",
        "\n",
        "    c2 = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(p1)\n",
        "    c2 = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(c2)\n",
        "    p2 = layers.MaxPooling2D((2, 2))(c2)\n",
        "\n",
        "    c3 = layers.Conv2D(256, (3, 3), activation='relu', padding='same')(p2)\n",
        "    c3 = layers.Conv2D(256, (3, 3), activation='relu', padding='same')(c3)\n",
        "    p3 = layers.MaxPooling2D((2, 2))(c3)\n",
        "\n",
        "    # Bottleneck\n",
        "    c4 = layers.Conv2D(512, (3, 3), activation='relu', padding='same')(p3)\n",
        "    c4 = layers.Conv2D(512, (3, 3), activation='relu', padding='same')(c4)\n",
        "\n",
        "    # Decoder\n",
        "    u5 = layers.Conv2DTranspose(256, (2, 2), strides=(2, 2), padding='same')(c4)\n",
        "    u5 = layers.concatenate([u5, c3])\n",
        "    c5 = layers.Conv2D(256, (3, 3), activation='relu', padding='same')(u5)\n",
        "\n",
        "    u6 = layers.Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(c5)\n",
        "    u6 = layers.concatenate([u6, c2])\n",
        "    c6 = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(u6)\n",
        "\n",
        "    u7 = layers.Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(c6)\n",
        "    u7 = layers.concatenate([u7, c1])\n",
        "    c7 = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(u7)\n",
        "\n",
        "    outputs = layers.Conv2D(1, (1, 1), activation='sigmoid')(c7)\n",
        "    return keras.Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "# Compile U-Net\n",
        "unet_model = build_unet()\n",
        "unet_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "print(\"U-Net model built and compiled.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K7tLCPxyocjf",
        "outputId": "b1b3c04d-6868-49fa-b625-5e88487c58e1"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "U-Net model built and compiled.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Image Preprocessing & Segmentation Functions"
      ],
      "metadata": {
        "id": "RxDdyrZFgFxB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TARGET_SIZE = (256, 256)\n",
        "\n",
        "def preprocess_image(image_path):\n",
        "    \"\"\"Load and resize RGB MRI image.\"\"\"\n",
        "    image = load_img(image_path, color_mode=\"rgb\")\n",
        "    image = image.resize(TARGET_SIZE)\n",
        "    return img_to_array(image) / 255.0\n",
        "\n",
        "def get_segmentation_mask(image):\n",
        "    \"\"\"Generate tumor segmentation mask using U-Net.\"\"\"\n",
        "    img_resized = tf.image.resize(image, TARGET_SIZE)\n",
        "    img_resized = tf.expand_dims(img_resized, 0)\n",
        "    mask = unet_model.predict(img_resized)[0]\n",
        "    return (mask > 0.5).astype(np.uint8)\n",
        "\n",
        "def mask_to_bbox(mask, is_no_tumor=False):\n",
        "    \"\"\"Generate bounding box from mask.\"\"\"\n",
        "    if is_no_tumor:\n",
        "        return [0, 0, TARGET_SIZE[0], TARGET_SIZE[1]]\n",
        "    y_indices, x_indices = np.where(mask > 0)\n",
        "    if y_indices.size == 0:\n",
        "        return [0, 0, TARGET_SIZE[0], TARGET_SIZE[1]]\n",
        "    return [np.min(x_indices), np.min(y_indices), np.max(x_indices), np.max(y_indices)]"
      ],
      "metadata": {
        "id": "gjJPZUXzolls"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Augmentation & Balancing"
      ],
      "metadata": {
        "id": "K3nS5pkPoola"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "augmentor = ImageDataGenerator(\n",
        "    rotation_range=40,\n",
        "    width_shift_range=0.3,\n",
        "    height_shift_range=0.3,\n",
        "    shear_range=0.3,\n",
        "    zoom_range=0.3,\n",
        "    horizontal_flip=True,\n",
        "    vertical_flip=True,\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "\n",
        "def augment_image(image, count, save_dir, tumor_class):\n",
        "    \"\"\"Augment image and save with unique names.\"\"\"\n",
        "    image = np.expand_dims(image, axis=0)\n",
        "    for i in range(count):\n",
        "        aug_img = next(augmentor.flow(image, batch_size=1))[0]\n",
        "        unique_id = str(uuid.uuid4())\n",
        "        save_path = os.path.join(save_dir, f\"{tumor_class}_aug_{unique_id}.png\")\n",
        "        Image.fromarray((aug_img * 255).astype(np.uint8)).save(save_path)"
      ],
      "metadata": {
        "id": "GnUuVZgMor2u"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Splitting"
      ],
      "metadata": {
        "id": "DshkYb-8owYa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def split_and_balance(images, max_count, save_dir, tumor_class):\n",
        "    \"\"\"Split and balance images across Train, Val, Test.\"\"\"\n",
        "    train, temp = train_test_split(images, test_size=0.3, random_state=42)\n",
        "    val, test = train_test_split(temp, test_size=0.5, random_state=42)\n",
        "\n",
        "    # Balance training set\n",
        "    current_count = len(train)\n",
        "    if current_count < max_count:\n",
        "        augment_image(train[0], max_count - current_count, save_dir, tumor_class)\n",
        "\n",
        "    return train, val, test"
      ],
      "metadata": {
        "id": "XTI8gp9boygT"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generating Annotations"
      ],
      "metadata": {
        "id": "SoE-Ljeho1dF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_pascal_voc_xml(image_path, bbox, label, save_dir):\n",
        "    \"\"\"Generate PASCAL VOC XML annotations.\"\"\"\n",
        "    image_name = os.path.basename(image_path)\n",
        "    xml_filename = os.path.splitext(image_name)[0] + \".xml\"\n",
        "\n",
        "    root = ET.Element(\"annotation\")\n",
        "    ET.SubElement(root, \"filename\").text = image_name\n",
        "    size = ET.SubElement(root, \"size\")\n",
        "    ET.SubElement(size, \"width\").text = str(TARGET_SIZE[0])\n",
        "    ET.SubElement(size, \"height\").text = str(TARGET_SIZE[1])\n",
        "\n",
        "    obj = ET.SubElement(root, \"object\")\n",
        "    ET.SubElement(obj, \"name\").text = label\n",
        "    bbox_elem = ET.SubElement(obj, \"bndbox\")\n",
        "    for tag, value in zip([\"xmin\", \"ymin\", \"xmax\", \"ymax\"], bbox):\n",
        "        ET.SubElement(bbox_elem, tag).text = str(value)\n",
        "\n",
        "    with open(os.path.join(save_dir, xml_filename), \"w\") as f:\n",
        "        f.write(parseString(ET.tostring(root)).toprettyxml())"
      ],
      "metadata": {
        "id": "TBPY6c6Vo3ZD"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset Preprocessing"
      ],
      "metadata": {
        "id": "j6D-cz2bo6cV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def process_dataset():\n",
        "    \"\"\"Preprocess images, generate masks, and save with annotations.\"\"\"\n",
        "    max_count = max(len(os.listdir(os.path.join(RAW_DATASET_PATH, cls))) for cls in tumor_classes)\n",
        "\n",
        "    for tumor_class in tumor_classes:\n",
        "        class_path = os.path.join(RAW_DATASET_PATH, tumor_class)\n",
        "        images = [os.path.join(class_path, img) for img in os.listdir(class_path)]\n",
        "        train, val, test = split_and_balance(images, max_count, class_path, tumor_class)\n",
        "\n",
        "        for split, dataset in zip([\"Train\", \"Val\", \"Test\"], [train, val, test]):\n",
        "            img_dest = os.path.join(OUTPUT_PATH, split, \"Images\", tumor_class)\n",
        "            ann_dest = os.path.join(OUTPUT_PATH, split, \"Annotations\", tumor_class)\n",
        "            os.makedirs(img_dest, exist_ok=True)\n",
        "            os.makedirs(ann_dest, exist_ok=True)\n",
        "\n",
        "            for img_path in dataset:\n",
        "                img = preprocess_image(img_path)\n",
        "                mask = get_segmentation_mask(img) if tumor_class != \"no_tumour\" else np.zeros(TARGET_SIZE[:2])\n",
        "                bbox = mask_to_bbox(mask)\n",
        "                unique_id = str(uuid.uuid4())\n",
        "                save_name = f\"{os.path.splitext(os.path.basename(img_path))[0]}_{unique_id}.png\"\n",
        "                save_path = os.path.join(img_dest, save_name)\n",
        "                shutil.copy(img_path, save_path)\n",
        "                create_pascal_voc_xml(save_path, bbox, tumor_class, ann_dest)\n",
        "\n",
        "                print(f\"{split}: {tumor_class} - {save_name}\")"
      ],
      "metadata": {
        "id": "JFnCvYz_o8Ib"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Class Distribution Visualization"
      ],
      "metadata": {
        "id": "--t2jcVrqqv7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def count_images_per_class(base_path):\n",
        "    \"\"\"Count images per tumor class.\"\"\"\n",
        "    class_counts = {}\n",
        "    for tumor_class in tumor_classes:\n",
        "        class_path = os.path.join(base_path, tumor_class)\n",
        "        num_images = len([img for img in os.listdir(class_path) if img.lower().endswith(('.png', '.jpg', '.jpeg'))])\n",
        "        class_counts[tumor_class] = num_images\n",
        "    return class_counts\n",
        "\n",
        "\n",
        "def plot_class_distributions(before_counts, after_counts):\n",
        "    \"\"\"Plot side-by-side bar charts of class distributions before and after balancing.\"\"\"\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(18, 6))\n",
        "\n",
        "    # Before Balancing\n",
        "    axes[0].bar(before_counts.keys(), before_counts.values(), color='skyblue')\n",
        "    axes[0].set_title(\"Class Distribution Before Balancing\")\n",
        "    axes[0].set_ylabel(\"Number of Images\")\n",
        "    axes[0].set_xlabel(\"Tumor Classes\")\n",
        "    axes[0].tick_params(axis='x', rotation=45)\n",
        "\n",
        "    # After Balancing\n",
        "    axes[1].bar(after_counts.keys(), after_counts.values(), color='salmon')\n",
        "    axes[1].set_title(\"Class Distribution After Balancing\")\n",
        "    axes[1].set_ylabel(\"Number of Images\")\n",
        "    axes[1].set_xlabel(\"Tumor Classes\")\n",
        "    axes[1].tick_params(axis='x', rotation=45)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "0t-_AOjYqt3n"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Main Function"
      ],
      "metadata": {
        "id": "PhKQfEyCqyk-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main_pipeline():\n",
        "    print(\"Starting preprocessing pipeline...\")\n",
        "\n",
        "    # Count classes before balancing\n",
        "    before_balancing_counts = count_images_per_class(RAW_DATASET_PATH)\n",
        "    print(\"Class counts before balancing:\", before_balancing_counts)\n",
        "\n",
        "    # Process dataset with U-Net & Balancing\n",
        "    process_dataset()\n",
        "\n",
        "    # Count classes after balancing\n",
        "    after_balancing_counts = count_images_per_class(os.path.join(OUTPUT_PATH, \"Train/Images\"))\n",
        "    print(\"Class counts after balancing:\", after_balancing_counts)\n",
        "\n",
        "    # Plot class distribution comparison\n",
        "    plot_class_distributions(before_balancing_counts, after_balancing_counts)\n",
        "\n",
        "    print(\"Dataset preprocessing complete and saved at:\", OUTPUT_PATH)\n",
        "\n",
        "\n",
        "# Execute the final pipeline\n",
        "main_pipeline()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Mco07gJ8q0Aw",
        "outputId": "b57ce179-c48c-4a56-dbf2-a362b71d8e17"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting preprocessing pipeline...\n",
            "Class counts before balancing: {'tuberculoma': 138, 'granuloma': 78, 'no_tumour': 2000, 'papiloma': 227, 'schwannoma': 453, 'meduloblastoma': 126, 'pituitary': 1757, 'neurocitoma': 457, 'oligodendroglioma': 220, 'meningioma': 1645, 'germinoma': 97, 'astrocitoma': 574, 'glioblastoma': 197, 'ependimoma': 150, 'ganglioglioma': 59, 'carcinoma': 186}\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "could not convert string to float: '/content/drive/MyDrive/DSGP/DSGP_dataset/tuberculoma/7b1e6ea5eb22b9e55a58357dd2ee04_big_gallery.jpeg'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-18c089c30bbf>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m# Execute the final pipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mmain_pipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-10-18c089c30bbf>\u001b[0m in \u001b[0;36mmain_pipeline\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m# Process dataset with U-Net & Balancing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mprocess_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m# Count classes after balancing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-4df2b53bdca0>\u001b[0m in \u001b[0;36mprocess_dataset\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mclass_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRAW_DATASET_PATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtumor_class\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplit_and_balance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtumor_class\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Train\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Val\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Test\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-0156c7adfd50>\u001b[0m in \u001b[0;36msplit_and_balance\u001b[0;34m(images, max_count, save_dir, tumor_class)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mcurrent_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcurrent_count\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mmax_count\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0maugment_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_count\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mcurrent_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtumor_class\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-ee618b71d046>\u001b[0m in \u001b[0;36maugment_image\u001b[0;34m(image, count, save_dir, tumor_class)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0maug_img\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maugmentor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0munique_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muuid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muuid4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0msave_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"{tumor_class}_aug_{unique_id}.png\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/legacy/preprocessing/image.py\u001b[0m in \u001b[0;36mflow\u001b[0;34m(self, x, y, batch_size, shuffle, sample_weight, seed, save_to_dir, save_prefix, save_format, ignore_class_split, subset)\u001b[0m\n\u001b[1;32m   1101\u001b[0m         \u001b[0msubset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m     ):\n\u001b[0;32m-> 1103\u001b[0;31m         return NumpyArrayIterator(\n\u001b[0m\u001b[1;32m   1104\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1105\u001b[0m             \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/legacy/preprocessing/image.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, image_data_generator, batch_size, shuffle, sample_weight, seed, data_format, save_to_dir, save_prefix, save_format, subset, ignore_class_split, dtype)\u001b[0m\n\u001b[1;32m    607\u001b[0m                     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msplit_idx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    608\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 609\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    610\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx_misc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_misc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    611\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: could not convert string to float: '/content/drive/MyDrive/DSGP/DSGP_dataset/tuberculoma/7b1e6ea5eb22b9e55a58357dd2ee04_big_gallery.jpeg'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualize Tumor with Bounding Box"
      ],
      "metadata": {
        "id": "9zWE4TLHq_lf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_tumor_with_bbox(image_path):\n",
        "    \"\"\"Visualize segmented tumor with bounding box.\"\"\"\n",
        "    image = cv2.imread(image_path)\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) / 255.0\n",
        "    mask = get_segmentation_mask(image)\n",
        "    bbox = mask_to_bbox(mask)\n",
        "    cv2.rectangle(image, (bbox[0], bbox[1]), (bbox[2], bbox[3]), (255, 0, 0), 2)\n",
        "    plt.imshow(image)\n",
        "    plt.title(\"Tumor Segmentation with Bounding Box\")\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "visualize_tumor_with_bbox()"
      ],
      "metadata": {
        "id": "kqwElAFQrFTU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}